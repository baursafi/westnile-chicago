{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble, preprocessing\n",
    "import datetime as dt\n",
    "from sklearn import model_selection, metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and building features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset \n",
    "train = pd.read_csv('asset/train.csv')\n",
    "test = pd.read_csv('asset/test.csv')\n",
    "sample = pd.read_csv('asset/sampleSubmission.csv')\n",
    "weather = pd.read_csv('asset/weather.csv')\n",
    "spray = pd.read_csv('asset/spray.csv')\n",
    "\n",
    "# Get labels\n",
    "labels = train.WnvPresent.values\n",
    "\n",
    "# Creating dummy variables for the weather data\n",
    "weather.CodeSum = weather.CodeSum.apply(str.split)\n",
    "for (i,list_) in enumerate(weather.CodeSum):\n",
    "    for item in list_:\n",
    "        if item not in weather.columns:\n",
    "            weather[item] = 0\n",
    "        weather.set_value(col=item,index=i,value=1)\n",
    "\n",
    "\n",
    "# Not using codesum anymore\n",
    "weather = weather.drop('CodeSum', axis=1)\n",
    "# Split station 1 and 2 and join horizontally\n",
    "weather_stn1 = weather[weather['Station']==1]\n",
    "weather_stn2 = weather[weather['Station']==2]\n",
    "weather_stn1 = weather_stn1.drop('Station', axis=1)\n",
    "weather_stn2 = weather_stn2.drop('Station', axis=1)\n",
    "weather = weather_stn1.merge(weather_stn2, on='Date')\n",
    "\n",
    "train.Date = pd.to_datetime(train.Date)\n",
    "test.Date = pd.to_datetime(test.Date)\n",
    "weather.Date = pd.to_datetime(weather.Date)\n",
    "spray.Date = pd.to_datetime(spray.Date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace some missing values and T with -1\n",
    "weather = weather.replace('M', -1)\n",
    "weather = weather.replace('-', -1)\n",
    "weather = weather.replace('T', 0)\n",
    "weather = weather.replace(' T', 0)\n",
    "weather = weather.replace('  T', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather['yoni_interaction_term'] = (weather.DewPoint_x)/(weather.Tavg_x.apply(float))\n",
    "#weather['yoni_interaction_term_y'] = (weather.DewPoint_y)/(weather.Tavg_y.apply(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taking derivities of some columns\n",
    "weather['tmax_diff'] = weather.Tmax_x.diff()\n",
    "weather['dewpoint_diff'] = weather.DewPoint_x.diff()\n",
    "weather['tmin_diff'] = weather.Tmin_x.diff()\n",
    "#weather['wetbulb_diff'] = weather.WetBulb_x().apply(float).diff()\n",
    "weather['precip_diff'] = weather.PrecipTotal_x.apply(float).diff()\n",
    "weather['tavg_diff'] = weather.Tavg_x.apply(float).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to extract month and day from dataset\n",
    "train['year'] = train['Date'].dt.year\n",
    "train['month'] = train['Date'].dt.month\n",
    "train['day'] = train['Date'].dt.day\n",
    "test['year'] = test['Date'].dt.year\n",
    "test['month'] = test['Date'].dt.month\n",
    "test['day'] = test['Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop address columns\n",
    "train = train.drop(['Address', 'AddressNumberAndStreet','WnvPresent', 'NumMosquitos'], axis = 1)\n",
    "test = test.drop(['Id', 'Address', 'AddressNumberAndStreet'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This method merges the weather df with a dataframe and does so mulitple times by days for the given amount\n",
    "# of days\n",
    "def merge_weather_previous_days(df, days, df_weather = weather):\n",
    "    for i in range(1, days):\n",
    "        date_col = 'Date'+ '_' + str(i)\n",
    "        if(i == 0):\n",
    "            date_col = 'Date'\n",
    "        df[date_col] = df.Date.apply(lambda a: a - pd.Timedelta(days=1))\n",
    "        df = df.merge(weather, left_on=date_col, right_on='Date', suffixes=(\"\",\"_day_\" + str(i)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_weather_previous_weeks(df, weeks, df_weather = weather):\n",
    "    for i in range(1, weeks):\n",
    "        date_col = 'Date'+ '_' + str(i)\n",
    "        if(i == 0):\n",
    "            date_col = 'Date'\n",
    "        df[date_col] = df.Date.apply(lambda a: a - pd.Timedelta(weeks=1))\n",
    "        df = df.merge(weather, left_on=date_col, right_on='Date', suffixes=(\"\",\"_week_\" + str(i)))\n",
    "    return df\n",
    "\n",
    "\n",
    "# #This method is attempting to take the mean of weather over a weekly period, and append that values to our DF\n",
    "# def agg_by_week(df, df_weather = weather):\n",
    "#     # grouping data in weather column by week and taking the average.\n",
    "#     # i'm going to use this data to build columns in the passed in data frame that has the average value\n",
    "#     # of the \n",
    "#     agg_weather = df_weather.set_index('Date').groupby(pd.TimeGrouper('W')).mean()\n",
    "#     for d in agg_weather.index:\n",
    "#         for c in agg_weather:\n",
    "#             if c not in df.columns:\n",
    "#                 df[c] = agg_weather.get_value(d, c)\n",
    "#             for j in range(1, len(df)):\n",
    "#                 if df.get_value(j, \"Date\") < d:\n",
    "#                     if df.get_value(j-1, \"Date\") > d:\n",
    "#                         df.set_value(j, c, agg_weather.get_value(d,c))\n",
    "#                 else:\n",
    "#                     break\n",
    "                    \n",
    "#This method is attempting to take the mean of weather over a weekly period, and append that values to our DF\n",
    "def agg_by_week(df, df_weather = weather):\n",
    "    # grouping data in weather column by week and taking the average.\n",
    "    # i'm going to use this data to build columns in the passed in data frame that has the average value\n",
    "    # of the \n",
    "    for i in range(0, len(df)):\n",
    "        df_date = df.get_value(i, 'Date')\n",
    "        agg_weather = df_weather.set_index('Date').groupby(pd.TimeGrouper('W')).mean()\n",
    "        for c in agg_weather.columns:\n",
    "            col_name = c+ \"_agg\"\n",
    "            if c.startswith('codesum'):\n",
    "                continue\n",
    "            if c not in df.columns:\n",
    "                df[col_name] = -1\n",
    "            value =  agg_weather[c][(agg_weather.index < df_date) & \n",
    "                                (agg_weather.index > df_date - pd.Timedelta(weeks=1))][0]\n",
    "            #print(value)\n",
    "            if np.isnan(value):\n",
    "                value = -1\n",
    "            df.set_value(i, col_name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Putting historical weather data onto the data frame\n",
    "train = merge_weather_previous_days(train, 3, weather)\n",
    "test =  merge_weather_previous_days(test, 4, weather)\n",
    "\n",
    "train = merge_weather_previous_weeks(train, 3, weather)\n",
    "test =  merge_weather_previous_weeks(test, 4, weather)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function allows you to create columns for the data_df input that will have a value of 1 or zero\n",
    "# If a trap has been sprayed with in the time period specified\n",
    "\n",
    "def create_sprayed_cols(data_df, spray_df, time_period=2):\n",
    "    # Iterating over unique dates that sprays took place\n",
    "    # Sprays took place over 10 days as trucks drove around chicago\n",
    "    for date in set(spray_df.Date):\n",
    "        # I only want data for this unique date\n",
    "        spray_temp = spray_df[spray_df.Date == date]\n",
    "        # Resetting index to make iterating easier\n",
    "        spray_temp.index = range(0, len(spray_temp))\n",
    "        \n",
    "        # I am creating a column for every unique date and initalizing it's rows to 0\n",
    "        # I will set these values to 1 when I find a trap that was sprayed\n",
    "        col_name = 'spray_'+date.strftime('%Y-%m-%d')+\"_\"+str(time_period)\n",
    "        data_df[col_name] = 0\n",
    "        # Iterating over each row of our training data to determine if a trap is in the location\n",
    "        # of a spray. I am also checking to see if the spray was in the past\n",
    "        for r in range(0,len(data_df)):\n",
    "            if data_df.get_value(r,'Date') > date and data_df.get_value(r,'Date') < date + pd.Timedelta(weeks=time_period):\n",
    "\n",
    "                # I am casting the lat and long to ints, and multiplaying by 100 to truncate precision\n",
    "                # In other words, I'm taking pin points and making them into squares\n",
    "                cur_lat = int(data_df.get_value(r, 'Latitude') * 100)\n",
    "                cur_long = int(data_df.get_value(r, 'Longitude') * 100)\n",
    "                \n",
    "                # Iterating over each value in my spray data\n",
    "                for i in range(0, len(spray_temp)):\n",
    "                    \n",
    "                    spray_lat = int(spray_temp.get_value(i,'Latitude')*100)\n",
    "                    spray_long = int(spray_temp.get_value(i,'Longitude')*100)\n",
    "                    \n",
    "                    latdiff = spray_lat - cur_lat\n",
    "                    longdiff = spray_long - cur_long\n",
    "                    dis = .5 **(latdiff ** 2 + longdiff ** 2)\n",
    "                    \n",
    "                    # I am now checking if something is in the square +/- some threshold\n",
    "                    if (cur_lat < spray_lat + 15 and cur_lat > spray_lat - 15) and \\\n",
    "                    (cur_long < spray_long + 15 and cur_long > spray_long - 15):\n",
    "                        data_df.set_value(r,col_name, 1)\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding spray data\n",
    "create_sprayed_cols(train, spray, time_period=2)\n",
    "create_sprayed_cols(test,spray, time_period=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert categorical data to numbers\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "lbl.fit(list(train['Species'].values) + list(test['Species'].values))\n",
    "train['Species'] = lbl.transform(train['Species'].values)\n",
    "test['Species'] = lbl.transform(test['Species'].values)\n",
    "\n",
    "lbl.fit(list(train['Street'].values) + list(test['Street'].values))\n",
    "train['Street'] = lbl.transform(train['Street'].values)\n",
    "test['Street'] = lbl.transform(test['Street'].values)\n",
    "\n",
    "lbl.fit(list(train['Trap'].values) + list(test['Trap'].values))\n",
    "train['Trap'] = lbl.transform(train['Trap'].values)\n",
    "test['Trap'] = lbl.transform(test['Trap'].values)\n",
    "\n",
    "# Converting data to catagoreical data. Instead of using the label encoder\n",
    "train = pd.get_dummies(train, prefix = ['Trap', 'Species', 'Block', 'Street'],\\\n",
    "                                      columns=['Trap','Species','Block','Street'])\n",
    "test = pd.get_dummies(test, prefix = ['Trap', 'Species','Block','Street'],\\\n",
    "                                     columns=['Trap','Species', 'Block', 'Street'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop columns with -1s\n",
    "train = train.loc[:,(train != -1).any(axis=0)]\n",
    "test = test.loc[:,(test != -1).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roland/anaconda3/envs/ga-immersive/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  \n",
      "/home/roland/anaconda3/envs/ga-immersive/lib/python2.7/site-packages/ipykernel_launcher.py:3: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Making everything numerical if it is a string\n",
    "train = train.convert_objects(convert_numeric=True)\n",
    "test = test.convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting data to polar coordinates. This may help location become a better predictor.\n",
    "\n",
    "epicenter_lat = 41.903002\n",
    "epicenter_long = -87.688267\n",
    "\n",
    "# epicenter_rho = np.sqrt(epicenter_lat**2 + epicenter_long**2)\n",
    "# epicenter_phi = np.tan(epicenter_long, epicenter_lat)\n",
    "\n",
    "train['rho'] = np.sqrt((train['Latitude'] - epicenter_lat)**2 + (train['Longitude'] - epicenter_long)**2)\n",
    "train['phi'] = np.arctan2((train['Latitude']- epicenter_lat),train['Longitude']- epicenter_long)\n",
    "test['rho'] = np.sqrt((test['Latitude'] - epicenter_lat)**2 + (test['Longitude'] - epicenter_long)**2)\n",
    "test['phi'] = np.arctan2((test['Latitude']- epicenter_lat),test['Longitude']- epicenter_long)\n",
    "train['phi_x_rho'] = train.rho * train.phi\n",
    "test['phi_x_rho'] = test.rho * test.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9616    -3.010102\n",
       "2884    -3.010102\n",
       "2883    -3.010102\n",
       "9437    -3.010102\n",
       "9438    -3.010102\n",
       "9439    -3.010102\n",
       "7602    -3.010102\n",
       "7603    -3.010102\n",
       "7130    -3.010102\n",
       "8737    -3.010102\n",
       "7131    -3.010102\n",
       "8736    -3.010102\n",
       "4386    -3.010102\n",
       "4502    -3.010102\n",
       "2636    -3.010102\n",
       "2635    -3.010102\n",
       "10346   -3.010102\n",
       "10345   -3.010102\n",
       "10344   -3.010102\n",
       "268     -3.010102\n",
       "269     -3.010102\n",
       "9930    -3.010102\n",
       "8591    -3.010102\n",
       "1036    -3.010102\n",
       "2885    -3.010102\n",
       "1037    -3.010102\n",
       "196     -3.010102\n",
       "4235    -3.010102\n",
       "4021    -3.010102\n",
       "4022    -3.010102\n",
       "           ...   \n",
       "3509     3.141593\n",
       "5375     3.141593\n",
       "1050     3.141593\n",
       "6363     3.141593\n",
       "7135     3.141593\n",
       "386      3.141593\n",
       "7390     3.141593\n",
       "5005     3.141593\n",
       "5006     3.141593\n",
       "5007     3.141593\n",
       "4121     3.141593\n",
       "7511     3.141593\n",
       "7262     3.141593\n",
       "6083     3.141593\n",
       "2641     3.141593\n",
       "2642     3.141593\n",
       "7136     3.141593\n",
       "5540     3.141593\n",
       "1370     3.141593\n",
       "3922     3.141593\n",
       "3921     3.141593\n",
       "6267     3.141593\n",
       "7261     3.141593\n",
       "7260     3.141593\n",
       "6266     3.141593\n",
       "1369     3.141593\n",
       "1368     3.141593\n",
       "3842     3.141593\n",
       "3923     3.141593\n",
       "5267     3.141593\n",
       "Name: phi, Length: 10506, dtype: float64"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.phi.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(train[list(filter(lambda a: a.startswith('Date'), train.columns))], axis=1)\n",
    "test = test.drop(test[list(filter(lambda a: a.startswith('Date'), test.columns))], axis=1)\n",
    "train = train.drop(['Latitude', 'Longitude'], axis=1)\n",
    "test = test.drop(['Latitude', 'Longitude'], axis=1)\n",
    "train = train.drop(set(train.columns) - set(test.columns), axis=1)\n",
    "test = test.drop(set(test.columns) - set(train.columns), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data to CSV to spped up future model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('train_w_weather.csv', index=False)\n",
    "test.to_csv('test_w_weather.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv('train_w_weather.csv')\n",
    "# test = pd.read_csv('test_w_weather.csv')\n",
    "\n",
    "# labels = train.WnvPresent.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid searching below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # method to help fit with xgboost\n",
    "\n",
    "# def modelfit(alg, X,y, predictors=None,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "#     if predictors == None:\n",
    "#         predictors = X.columns\n",
    "#     if useTrainCV:\n",
    "#         xgb_param = alg.get_xgb_params()\n",
    "#         xgtrain = xgb.DMatrix(X[predictors].values, label=y)\n",
    "#         cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "#             metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "#         alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "#     #Fit the algorithm on the data\n",
    "#     alg.fit(X[predictors].values,y,eval_metric='auc')\n",
    "        \n",
    "#     #Predict training set:\n",
    "#     dtrain_predictions = alg.predict(X[predictors].values)\n",
    "#     dtrain_predprob = alg.predict_proba(X[predictors].values)[:,1]\n",
    "        \n",
    "#     #Print model report:\n",
    "#     print (\"\\nModel Report\")\n",
    "#     print (\"Accuracy : %.4g\" % metrics.accuracy_score(y, dtrain_predictions))\n",
    "#     print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, dtrain_predprob))\n",
    "                    \n",
    "#     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     print(feat_imp)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "# xgb1 = xgb.XGBClassifier(\n",
    "#  learning_rate =0.1,\n",
    "#  n_estimators=1000,\n",
    "#  max_depth=5,\n",
    "#  min_child_weight=1,\n",
    "#  gamma=0,\n",
    "#  subsample=0.8,\n",
    "#  colsample_bytree=0.8,\n",
    "#  objective= 'binary:logistic',\n",
    "#  nthread=4,\n",
    "#  scale_pos_weight=1,\n",
    "#  seed=27)\n",
    "#modelfit(xgb1, train_xgb, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] colsample_bytree=0.8, silent=1, learning_rate=0.05, nthread=6, min_child_weight=1, n_estimators=3000, subsample=0.8, seed=1337, objective=binary:logistic, max_depth=2 \n",
      "[CV] colsample_bytree=0.8, silent=1, learning_rate=0.05, nthread=6, min_child_weight=1, n_estimators=3000, subsample=0.8, seed=1337, objective=binary:logistic, max_depth=2 \n",
      "[CV] colsample_bytree=0.8, silent=1, learning_rate=0.05, nthread=6, min_child_weight=1, n_estimators=3000, subsample=0.8, seed=1337, objective=binary:logistic, max_depth=2 \n",
      "[CV] colsample_bytree=0.8, silent=1, learning_rate=0.05, nthread=6, min_child_weight=1, n_estimators=3000, subsample=0.8, seed=1337, objective=binary:logistic, max_depth=2 \n",
      "[CV] colsample_bytree=0.8, silent=1, learning_rate=0.05, nthread=6, min_child_weight=1, n_estimators=3000, subsample=0.8, seed=1337, objective=binary:logistic, max_depth=2 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = train.columns\n",
    "#features = best_features\n",
    "xgb1 = xgb.XGBClassifier()\n",
    "#brute force scan for all parameters, here are the tricks\n",
    "#usually max_depth is 6,7,8\n",
    "#learning rate is around 0.05, but small changes may make big diff\n",
    "#tuning min_child_weight subsample colsample_bytree can have \n",
    "#much fun of fighting against overfit \n",
    "#n_estimators is how many round of boosting\n",
    "#finally, ensemble xgboost with multiple seeds may reduce variance\n",
    "parameters = {'nthread':[6], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05], #so called `eta` value\n",
    "              'max_depth': [2],\n",
    "              'min_child_weight': [1],\n",
    "              'silent': [1],\n",
    "              'subsample': [.8],\n",
    "              'colsample_bytree': [.8],\n",
    "              'n_estimators': [3000], #number of trees, change it to 1000 for better results\n",
    "              #'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "\n",
    "clf = model_selection.GridSearchCV(xgb1, parameters, n_jobs=6, \n",
    "                   cv=model_selection.StratifiedKFold(n_splits=5, random_state=None, shuffle=True\n",
    "                                                     ), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clf.fit(train[features], labels)\n",
    "model = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#trust your CV!\n",
    "best_parameters, score, _ = max(clf.grid_scores_, key=lambda x: x[1])\n",
    "print('Raw AUC score:', score)\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "test_probs = clf.predict_proba(test[features])[:,1]\n",
    "\n",
    "predictions = clf.predict_proba(test[features])[:,1]\n",
    "sample['WnvPresent'] = predictions\n",
    "sample.to_csv('beat_the_benchmark.csv', index=False)\n",
    "model = clf.best_estimator_\n",
    "# sample = pd.read_csv('../input/sample_submission.csv')\n",
    "# sample.QuoteConversion_Flag = test_probs\n",
    "# sample.to_csv(\"xgboost_best_parameter_submission.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # fit model no training data\n",
    "# model = xgb.XGBClassifier()\n",
    "# model.fit(X=train.values, y=labels, eval_metric='auc')\n",
    "\n",
    "\n",
    "# model.score(X=train.values, y=labels)\n",
    "# # param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "\n",
    "# # # specify validations set to watch performance\n",
    "# # watchlist  = [(labels,'eval'), \n",
    "# #               (train.values,'train')]\n",
    "# # num_round = 2\n",
    "# # bst = xgb.train(param, \n",
    "# #                 train.values, \n",
    "# #                 num_round, \n",
    "# #                 watchlist)\n",
    "\n",
    "# cvresult = xgb.cv(xgb_param, test, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "#             metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "#         alg.set_params(n_estimators=cvresult.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # n_estimators = [500]\n",
    "# # criterion = ['entropy']\n",
    "# # max_depth = [2,4,7]\n",
    "# # min_samples_split = [2,3]\n",
    "# # max_features = ['auto', 'log2',50]\n",
    "# # clf = ensemble.RandomForestClassifier()\n",
    "# # Random Forest Classifier \n",
    "# # grid = model_selection.GridSearchCV(estimator=clf, scoring='roc_auc', param_grid=dict(n_estimators=n_estimators, criterion=criterion,\n",
    "# #                                                     max_depth=max_depth, max_features=max_features, min_samples_split=min_samples_split))\n",
    "\n",
    "# param_grid = {'Cs': [[0.001, 0.01, 0.1, 1, 10, 100, 1000]]}\n",
    "# grid = model_selection.GridSearchCV(linear_model.LogisticRegressionCV(penalty='l2',scoring='roc_auc', class_weight='balanced'), param_grid)\n",
    "# grid.fit(train,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #grid.best_estimator_.fit(train, labels)\n",
    "\n",
    "# # create predictions and submission file\n",
    "# #predictions = grid.best_estimator_.predict_proba(test)[:,1]\n",
    "# predictions = xgb1.predict_proba(test.values)\n",
    "# sample['WnvPresent'] = predictions\n",
    "# sample.to_csv('beat_the_benchmark.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(model_selection.cross_val_score(model, X=train, y=labels, scoring='roc_auc', n_jobs=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "AddressAccuracy: 0.0285256039351\n",
      "WetBulb_x: 0.0147783253342\n",
      "tavg_diff: 0.0136327184737\n",
      "yoni_interaction_term: 0.0116851869971\n",
      "AvgSpeed_y: 0.0106541411951\n",
      "ResultDir_x_week_1: 0.00905029196292\n",
      "Block_52: 0.0087066097185\n",
      "DewPoint_x_week_1: 0.00824836734682\n",
      "Block_22: 0.00813380721956\n",
      "Species_2: 0.00779012497514\n",
      "ResultSpeed_y: 0.00767556438223\n",
      "phi: 0.00710276095197\n",
      "rho: 0.00675907870755\n",
      "Trap_133: 0.00664451811463\n",
      "WetBulb_x_week_1: 0.00664451811463\n",
      "dewpoint_diff: 0.00652995752171\n",
      "yoni_interaction_term_week_1: 0.00572803290561\n",
      "AvgSpeed_x_week_1: 0.00526979053393\n",
      "StnPressure_y: 0.00446786591783\n",
      "Trap_147: 0.00435330485925\n",
      "TS_x: 0.00423874426633\n",
      "AvgSpeed_x: 0.00389506248757\n",
      "Tmin_x_week_1: 0.00378050166182\n",
      "Trap_4: 0.00355138047598\n",
      "precip_diff: 0.00343681988306\n",
      "precip_diff_week_1: 0.00309313787147\n",
      "AvgSpeed_y_week_1: 0.00309313787147\n",
      "Tmin_y_week_1: 0.00297857704572\n",
      "Block_11: 0.0028640164528\n",
      "Tmax_x_week_1: 0.0028640164528\n",
      "day: 0.00274945585988\n",
      "ResultDir_x: 0.00263489526697\n",
      "Trap_22: 0.00252033444121\n",
      "Block_58: 0.00252033444121\n",
      "tavg_diff_week_1: 0.00229121325538\n",
      "ResultSpeed_x: 0.00229121325538\n",
      "TS_y_week_2: 0.00206209183671\n",
      "Trap_87: 0.00206209183671\n",
      "Sunrise_x: 0.00194753124379\n",
      "TSRA_x_week_1: 0.00183297053445\n",
      "RA_x_day_2: 0.00171840994153\n",
      "Trap_44: 0.00171840994153\n",
      "DZ_y_day_2: 0.0016038492322\n",
      "RA_y: 0.00148928852286\n",
      "RA_x_week_1: 0.00137472792994\n",
      "Block_51: 0.00126016722061\n",
      "Tmax_x: 0.00114560662769\n",
      "TSRA_y_week_2: 0.00103104591835\n",
      "VCTS_x_week_1: 0.000916485267226\n",
      "Trap_68: 0.000916485267226\n",
      "Tavg_x_week_1: 0.000801924616098\n",
      "Street_69: 0.000687363964971\n",
      "SeaLevel_y_week_1: 0.000687363964971\n",
      "StnPressure_x: 0.000687363964971\n",
      "Block_12: 0.000572803313844\n",
      "Street_36: 0.000572803313844\n",
      "Block_10: 0.000572803313844\n",
      "PrecipTotal_y_week_1: 0.000572803313844\n",
      "Block_33: 0.000458242633613\n",
      "Trap_11: 0.000458242633613\n",
      "Trap_89: 0.000458242633613\n",
      "Block_13: 0.000458242633613\n",
      "Tavg_x: 0.000458242633613\n",
      "tmin_diff: 0.000458242633613\n",
      "ResultDir_y: 0.000458242633613\n",
      "Trap_85: 0.000343681982486\n",
      "Species_6: 0.000343681982486\n",
      "Trap_1: 0.000343681982486\n",
      "SeaLevel_y: 0.000343681982486\n",
      "Species_3: 0.000343681982486\n",
      "spray_2: 0.000229121316806\n",
      "Street_18: 0.000229121316806\n",
      "Trap_64: 0.000229121316806\n",
      "Block_37: 0.000229121316806\n",
      "Block_61: 0.000229121316806\n",
      "PrecipTotal_x: 0.000229121316806\n",
      "DewPoint_x: 0.000229121316806\n",
      "Street_112: 0.000114560658403\n",
      "Block_17: 0.000114560658403\n",
      "Street_40: 0.000114560658403\n",
      "PrecipTotal_y: 0.000114560658403\n",
      "DewPoint_y: 0.000114560658403\n",
      "SeaLevel_x_week_1: 0.000114560658403\n",
      "ResultDir_y_week_1: 0.000114560658403\n",
      "Species_1: 0.000114560658403\n",
      "BR_x: 0.0\n",
      "Block_89: 0.0\n",
      "VCTS_x: 0.0\n",
      "HZ_y_week_1: 0.0\n",
      "TSRA_y_day_2: 0.0\n",
      "BR_y: 0.0\n",
      "RA_y_week_1: 0.0\n",
      "HZ_x: 0.0\n",
      "Block_71: 0.0\n",
      "HZ_y: 0.0\n",
      "BR_y_week_2: 0.0\n",
      "Street_102: 0.0\n",
      "BR_x_week_2: 0.0\n",
      "Trap_10: 0.0\n",
      "Trap_79: 0.0\n",
      "TS_y: 0.0\n",
      "HZ_x_week_1: 0.0\n",
      "Street_34: 0.0\n",
      "Street_19: 0.0\n",
      "Trap_136: 0.0\n",
      "Street_86: 0.0\n",
      "Block_39: 0.0\n",
      "Street_64: 0.0\n",
      "Trap_13: 0.0\n",
      "Block_35: 0.0\n",
      "Sunset_x_week_2: 0.0\n",
      "Trap_93: 0.0\n",
      "Block_82: 0.0\n",
      "Block_65: 0.0\n",
      "Block_40: 0.0\n",
      "SeaLevel_x: 0.0\n",
      "Street_50: 0.0\n",
      "WetBulb_y_day_2: 0.0\n",
      "StnPressure_y_week_1: 0.0\n",
      "Block_50: 0.0\n",
      "Tmax_y: 0.0\n",
      "WetBulb_y_week_1: 0.0\n",
      "DewPoint_y_week_1: 0.0\n",
      "Block_70: 0.0\n",
      "Block_42: 0.0\n",
      "PrecipTotal_x_week_1: 0.0\n",
      "Tavg_y: 0.0\n",
      "Trap_148: 0.0\n",
      "StnPressure_x_week_1: 0.0\n",
      "year: 0.0\n",
      "Tavg_y_week_1: 0.0\n",
      "Tmax_y_week_1: 0.0\n",
      "Depart_x_week_1: 0.0\n",
      "Depart_x: 0.0\n",
      "Tmin_y_day_2: 0.0\n",
      "dewpoint_diff_week_1: 0.0\n",
      "tmax_diff_week_1: 0.0\n",
      "Tmin_x: 0.0\n",
      "tmax_diff: 0.0\n",
      "tmin_diff_week_1: 0.0\n",
      "ResultSpeed_y_week_1: 0.0\n",
      "ResultSpeed_x_week_1: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa8e9f03d50>"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAD4CAYAAADVTSCGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMCxrEfRW14kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa92a5f3c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zipping feature importances and sorting by how importnat they are\n",
    "#importances = list(zip(train.columns,model.feature_importances_))\n",
    "importances = list(zip(best_features,model.feature_importances_))\n",
    "importances.sort(key=lambda a: a[1])\n",
    "importances = importances[::-1]\n",
    "# std = np.std([model.feature_importances_ for tree in model.estimators_],\n",
    "#              axis=0)\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(0, len(importances)):\n",
    "    print(\"{}: {}\".format(importances[f][0], importances[f][1]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "sns.barplot(x=train.columns, y=model.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_features = [ x[0] for x in list(filter(lambda a : a[1] >0, importances))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(best_features) - set(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in set(best_features) - set(train.columns):\n",
    "    best_features.remove(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(best_features) - set(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_features.append('spray_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
